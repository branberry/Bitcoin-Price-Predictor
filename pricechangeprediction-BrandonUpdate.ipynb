{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function retrieves the previous 4 days given a single day for input from the .csv file.\n",
    "def getEvalData(nparray,date,predict_delta):\n",
    "    res = [[]]\n",
    "    idx = np.argwhere(nparray==date)[0][0] - 5\n",
    "    # if the given date does not have 5 previous values...\n",
    "    if idx < 4 or idx + 7 > nparray.shape[0]:\n",
    "        return\n",
    "    else:\n",
    "        for i in range(5):\n",
    "            res[0].append(nparray[i+idx][1])\n",
    "        predict_day = nparray[i+idx+predict_delta][1]\n",
    "        predict_day_percent = (predict_day - res[0][4])/predict_day\n",
    "        res.append(predict_day_percent)\n",
    "        res.append(predict_day)\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576\n",
      "518\n",
      "439\n",
      "[[ 134.21  144.54  139.    116.99  105.21]\n",
      " [ 144.54  139.    116.99  105.21   97.75]\n",
      " [ 139.    116.99  105.21   97.75  112.5 ]\n",
      " ...\n",
      " [2608.56 2518.66 2571.34 2518.44 2372.56]\n",
      " [2518.66 2571.34 2518.44 2372.56 2337.79]\n",
      " [2571.34 2518.44 2372.56 2337.79 2398.84]]\n"
     ]
    }
   ],
   "source": [
    "# Trying to convert the input data, which starts off as dataframeX, must take 5 days and then transpose it\n",
    "# and then convert it into a numpy array\n",
    "\n",
    "rawdata = pd.read_csv('data/bitcoin.csv',usecols=[8])\n",
    "days = 5                                                      # Number of days we are looking at\n",
    "predict_delta = 7                                             # Number of days we are predicting into the future \n",
    "number_of_items = rawdata.shape[0] - (predict_delta + days)   # number of tests\n",
    "# size of our test \n",
    "test_size = number_of_items - 733\n",
    "\n",
    "# five day blocks in a list\n",
    "Xtrain = []    \n",
    "# the values of the day we are trying to predict\n",
    "Ytrain = []                                                   \n",
    "\n",
    "\n",
    "\n",
    "# Splits the array of closing prices up into multiple arrays, each holding five days worth of prices\n",
    "# then transposes each array and finally appends them onto one large matrix\n",
    "countup = 0\n",
    "counteven = 0\n",
    "countdown = 0\n",
    "for i in range(number_of_items - 200): \n",
    "    # finding the predict day\n",
    "    predict = rawdata.head(days + i + predict_delta).tail(1).as_matrix().transpose().tolist()[0][0]\n",
    "    last_day = rawdata.head(days + i).tail(1).as_matrix().transpose().tolist()[0][0]\n",
    "    # Converting and adding data to arrays\n",
    "    res = rawdata.head(days + i).tail(days).as_matrix().transpose().tolist()[0]\n",
    "    Xtrain.append(res)\n",
    "    y_val = (predict - last_day)/predict\n",
    "    if y_val > .03:\n",
    "        countup = countup + 1\n",
    "        Ytrain.append(np.array([1, 0, 0]))\n",
    "        \n",
    "    elif y_val > -.03 and y_val < .03:\n",
    "        counteven = counteven + 1\n",
    "        Ytrain.append(np.array([0, 1, 0]))\n",
    "        \n",
    "    else:\n",
    "        countdown = countdown + 1\n",
    "        Ytrain.append(np.array([0, 0, 1]))\n",
    "        \n",
    "    \n",
    "print(countup)\n",
    "print(counteven)\n",
    "print(countdown)\n",
    "# converts python arrays into numpy arrays\n",
    "Xtrain = np.array(Xtrain)\n",
    "Ytrain = np.array(Ytrain)\n",
    "print(Xtrain)\n",
    "\n",
    "#print(Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  387.49   402.97   391.73   392.15   394.97]\n",
      " [  402.97   391.73   392.15   394.97   380.29]\n",
      " [  391.73   392.15   394.97   380.29   379.47]\n",
      " ...\n",
      " [10931.4  10868.4  11359.4  11259.4  11171.4 ]\n",
      " [10868.4  11359.4  11259.4  11171.4  11440.7 ]\n",
      " [11359.4  11259.4  11171.4  11440.7  11786.3 ]]\n",
      "[-6.76307609e-02  2.38712493e-02  1.83158712e-02 -4.59470944e-03\n",
      "  2.08432903e-02  1.04431651e-03  4.20179241e-03  3.06563605e-02\n",
      " -2.61820097e-02 -5.95950658e-03  3.91466340e-02  7.51663679e-02\n",
      "  6.68182613e-02  7.72043486e-02  8.32772867e-02  1.01143547e-01\n",
      "  8.68128995e-02  1.03623387e-01  7.19462170e-02  8.58023986e-02\n",
      "  3.14921329e-02  2.03082716e-02  5.11141471e-03  2.62871688e-02\n",
      " -1.07278276e-02 -1.22260669e-02 -1.14233493e-04  3.30483545e-02\n",
      " -2.26420434e-03 -6.85402585e-03 -5.16133742e-02 -7.97613401e-02\n",
      " -6.32557455e-02 -5.64298127e-02 -5.10906587e-02 -2.20074242e-02\n",
      " -1.08359504e-02  2.54926605e-02  2.68451484e-02  1.53359417e-02\n",
      "  5.09076938e-03  6.86131037e-03  5.15575166e-03  8.29727545e-03\n",
      " -2.96422903e-02 -2.87496345e-03 -7.49244713e-04 -7.57300815e-03\n",
      "  3.01370518e-03  2.46387905e-03 -1.01587454e-02  1.82894674e-02\n",
      "  1.79451596e-02  3.04855188e-02  2.57407538e-02 -3.76932680e-03\n",
      " -7.76240297e-03  8.15875987e-04  1.86620729e-03  6.96176967e-03\n",
      " -1.39225469e-02 -6.62015945e-03  1.77110110e-02  2.02876644e-02\n",
      "  1.42400946e-02  5.68573808e-03 -3.48108057e-03  1.56561344e-03\n",
      "  2.46165499e-03  2.72819210e-03  7.55197885e-04  3.60610917e-03\n",
      "  2.17821321e-02  2.59191305e-02  1.36640150e-02  1.42560489e-02\n",
      "  2.36963560e-02  4.00099685e-02  5.59597713e-02  3.59626688e-02\n",
      "  4.37727636e-02  6.79518493e-02  7.11700583e-02  6.56096462e-02\n",
      "  7.42089995e-03 -9.35391194e-04  2.05669084e-02 -4.37187723e-03\n",
      " -1.47826857e-02 -3.76908719e-02 -3.50655119e-02  4.54423352e-03\n",
      " -2.29920979e-03  9.79112272e-03  2.22881319e-02  1.45458511e-02\n",
      "  3.43337387e-02  1.30852314e-03  1.32750204e-02  1.49306243e-02\n",
      " -8.62466258e-03 -6.29841771e-03 -2.14174880e-03 -1.39158006e-02\n",
      "  6.36872493e-03  4.15731820e-03 -3.66073260e-02 -2.93439957e-02\n",
      " -2.81594801e-02 -4.15414732e-02 -2.25374310e-02 -1.74895735e-02\n",
      " -1.11654804e-02  3.23569633e-02  6.50107718e-02  1.63855558e-01\n",
      "  1.65155920e-01  1.68040310e-01  1.60729408e-01  1.62631304e-01\n",
      "  1.57239251e-01  1.68186370e-01  7.45377403e-02  8.47855578e-02\n",
      "  8.82604092e-02  7.84079084e-02  7.68860464e-02  6.37975741e-02\n",
      "  1.43384072e-02  5.60381059e-02  1.45366985e-01  1.68715750e-01\n",
      "  1.58935761e-01  1.62469221e-01  2.50133758e-01  2.28919363e-01\n",
      "  1.97691179e-01  1.19144256e-01  4.45586859e-02 -2.83657091e-02\n",
      " -1.64983560e-01 -2.28100260e-01 -1.25672629e-01 -1.36982800e-01\n",
      " -2.13562769e-01 -1.25078212e-01 -3.03709428e-02  6.84023817e-02\n",
      "  7.33062049e-02  1.62649712e-02  5.48244991e-02  4.44690736e-02\n",
      "  4.15264898e-02  3.52355248e-02  5.52758626e-02 -5.11739728e-02\n",
      " -1.46732281e-02 -8.10188030e-02 -1.43217938e-02 -5.55847204e-02\n",
      " -9.14904823e-03 -3.49290265e-02  2.66229030e-02 -4.93026762e-03\n",
      "  1.48463157e-02  4.42998852e-02  3.78095705e-02  1.23502660e-02\n",
      "  1.68399231e-02  1.04208959e-02 -1.94122529e-02 -7.94740375e-03\n",
      " -2.74921365e-02 -2.90628344e-02 -3.23422014e-02 -1.73148926e-02\n",
      " -1.52359434e-02  9.69573357e-03 -7.78566522e-04 -5.85899981e-02\n",
      " -7.88922427e-02 -1.90530988e-01 -1.55360548e-01 -1.32701586e-01\n",
      " -1.42511825e-01 -1.14447582e-01 -5.39742530e-02 -2.57507825e-02\n",
      "  6.86117727e-02  4.34723864e-02  1.83833514e-02  2.13084621e-02\n",
      " -3.73981796e-03 -3.89503392e-02 -4.19751781e-02 -1.79412580e-02\n",
      " -3.29367433e-02 -2.57696058e-02 -2.07251186e-02 -6.68729586e-03\n",
      "  1.86475375e-02  3.32509587e-02  1.02329408e-02  1.19962770e-02\n",
      "  5.95402935e-03  6.93521953e-03 -2.06158435e-02 -1.28940078e-02\n",
      " -2.20166867e-02 -1.02337662e-02 -8.18461432e-03 -9.54045081e-03\n",
      " -7.14111964e-03  4.72409355e-02  5.70461528e-02  5.35452282e-02\n",
      "  5.39610773e-02  6.35760081e-02  8.62498403e-02  7.59721286e-02\n",
      "  4.05767349e-02 -3.14807489e-03  2.71274497e-03 -1.96966713e-03\n",
      " -6.32082269e-03 -3.15737462e-02 -2.61792181e-02 -2.89283475e-02\n",
      "  5.16503517e-03  1.62500205e-03 -1.52882576e-03 -2.26576237e-02\n",
      " -1.81955392e-02 -6.85090571e-03 -5.55896653e-03 -1.50458532e-02\n",
      " -1.95710808e-03 -3.53036277e-03  1.25345195e-02  1.55029801e-02\n",
      "  1.13162988e-02  1.84859442e-02  1.64677765e-02  6.68158724e-03\n",
      "  6.60439200e-03  1.27018334e-02  1.19571955e-02  1.19587763e-02\n",
      "  8.28608809e-03  9.50141873e-03  1.10825700e-02  4.81538678e-02\n",
      "  3.72215847e-02  3.73278475e-02  3.63221837e-02  3.05957880e-02\n",
      "  3.87762418e-02  3.16024969e-02 -4.87491379e-03 -8.99257756e-03\n",
      " -9.39986685e-03 -1.19305343e-02  2.83588675e-02  2.34982574e-02\n",
      "  2.22864660e-02  2.98514272e-02  7.04408079e-02  8.34652991e-02\n",
      "  8.23896179e-02  8.00442280e-02  6.38161457e-02  6.73495299e-02\n",
      "  9.89325696e-02  8.44053292e-02  5.66284304e-04  1.93248393e-02\n",
      " -1.57231810e-02  1.35765685e-02  3.07197815e-03 -2.80904416e-02\n",
      " -2.42786235e-02  3.74966808e-02  1.83833280e-02  2.31189277e-03\n",
      " -1.35179408e-02  2.68077501e-03  2.48728254e-03  2.81241602e-02\n",
      "  3.43464061e-02  4.68074349e-02  6.19595008e-02  3.96700546e-02\n",
      "  4.63036862e-02  5.28781527e-02  5.23778187e-04 -9.32067163e-04\n",
      " -1.34025484e-02 -2.20838206e-02  1.36606423e-03 -4.67511994e-03\n",
      " -2.14110930e-02  1.47514383e-03  2.17767618e-02  4.66488418e-02\n",
      "  4.63852688e-02  5.40659284e-02  3.01700277e-02  3.74499490e-02\n",
      "  2.92138050e-02  1.82146054e-02 -6.66416491e-03  4.51816950e-03\n",
      " -5.37850935e-03  2.74199131e-02  2.09336886e-02  1.70829708e-02\n",
      "  9.35624414e-03  1.54412608e-02  2.04595172e-02  2.63114619e-02\n",
      "  1.59200717e-02  2.53720907e-02  6.32881047e-02  9.99953733e-02\n",
      "  1.48669169e-01  1.20146414e-01  1.17889263e-01  1.26596225e-01\n",
      "  1.41791685e-01  1.45134847e-01  1.11926040e-01  4.08430777e-02\n",
      "  6.73625667e-02  1.02320876e-01  1.11710301e-01  1.05993256e-01\n",
      "  1.54850052e-01  3.93534508e-02 -6.54400355e-02 -6.06984448e-02\n",
      " -9.56211589e-02 -1.31719150e-01 -1.50008814e-01 -4.84686793e-01\n",
      " -2.59123045e-01 -9.49294886e-02 -1.10189269e-01 -1.08785593e-01\n",
      " -8.57455534e-02  2.86362535e-04  1.22780898e-01  1.04819425e-01\n",
      "  7.93828140e-02  1.12151358e-01  1.11250500e-01  9.71542111e-02\n",
      " -1.70831980e-02  1.65494598e-02  2.01833063e-02  2.68768687e-02\n",
      " -2.17016244e-04 -5.62262099e-03 -6.84499880e-04  8.00803792e-02\n",
      "  8.84511941e-02  9.31112868e-02  1.06960802e-01  1.16319877e-01\n",
      "  1.04970117e-01  1.13442181e-01  8.56927498e-02  6.96567489e-02\n",
      " -1.75184537e-02 -4.17126038e-02 -3.82796555e-02 -2.81831102e-02\n",
      " -4.79588953e-02 -5.65427306e-02 -5.51772740e-02  3.21770614e-02\n",
      "  5.49985185e-02  4.73909827e-02  4.64656875e-02  8.27237541e-02\n",
      "  9.93006366e-02  9.84034937e-02  1.19377400e-01  1.08607116e-01\n",
      "  7.81752693e-02  1.00695160e-01  8.47394425e-02  5.48064781e-02\n",
      "  8.59386503e-02  6.73775589e-02  7.94594467e-02  8.86826276e-02\n",
      "  8.04343709e-02  7.29555400e-02  3.56097880e-02 -6.30434783e-02\n",
      " -5.26045655e-02 -1.41727559e-01 -6.74587313e-02 -3.74494424e-02\n",
      " -3.32083252e-02  1.32741935e-02  7.97128704e-02 -5.72482131e-04\n",
      " -1.49877753e-02 -2.07440800e-01 -1.78096726e-01 -1.68549557e-01\n",
      " -1.06609313e-01 -1.91080313e-01 -1.43675560e-01 -1.73553631e-01\n",
      " -1.06910093e-03 -7.24193932e-02 -8.08973292e-03 -7.00854701e-02\n",
      " -8.81756205e-03 -1.18468868e-02  1.25276407e-01  9.96945858e-02\n",
      "  1.22884854e-01  8.57135363e-02  7.59761747e-02  7.54014118e-02\n",
      "  1.32115196e-01  8.93109015e-02  8.11684170e-02  7.21459419e-02\n",
      "  3.64913699e-02  5.95513730e-02  6.29722502e-02 -1.14600438e-02\n",
      " -8.01685595e-03 -2.92532324e-03 -4.16758246e-03  5.67882001e-03\n",
      "  5.49654609e-03  8.19638269e-03  4.86542780e-02  4.46053762e-02\n",
      "  4.80551429e-02  2.01042072e-02  4.49866016e-02  4.25289809e-02\n",
      "  5.52580635e-02  6.72747831e-02  7.17291565e-02  6.81500087e-02\n",
      "  1.04370535e-01  1.20603545e-01  1.28942333e-01  1.40266695e-01\n",
      "  1.43034591e-01  1.53633997e-01  1.62788194e-01  1.55832931e-01\n",
      "  1.75095018e-01  1.72352110e-01  1.66210628e-01  1.68184056e-01\n",
      "  9.78924048e-02  1.25274944e-01  1.17308213e-01  8.67449365e-03\n",
      " -1.20556949e-02  2.82531034e-02  2.12215074e-02  1.32549517e-01\n",
      "  1.34223616e-01  1.13800705e-01  2.00133431e-01  2.52527560e-01\n",
      "  2.47397325e-01  1.80621958e-01  9.74882175e-02 -2.24928514e-02\n",
      "  5.31589201e-02  3.64469035e-02 -6.66292801e-02 -6.87671940e-02\n",
      "  4.27346878e-02  1.14978602e-01  1.89428907e-01  1.41734446e-01\n",
      "  1.60487716e-01  2.40196284e-01  1.63149303e-01  1.41765456e-01\n",
      "  1.18726118e-01  1.46676573e-01  1.50873362e-01 -1.02194666e-02\n",
      " -5.38015914e-02 -9.00864597e-02 -1.38376519e-01 -1.21200210e-01\n",
      " -1.09880718e-01 -1.60821571e-01 -2.70427865e-02  1.75252316e-03\n",
      "  6.79521029e-02  8.90179307e-02  8.24617201e-02 -1.80778313e-02\n",
      "  1.58800653e-02 -4.48465775e-02 -6.63441008e-02 -4.43958536e-02\n",
      " -6.54072744e-02 -1.06443785e-01 -7.15409419e-02 -3.30903621e-02\n",
      "  3.33884542e-02  1.89073046e-02  1.04535375e-02  2.65433803e-02\n",
      "  1.50159212e-02  5.31979435e-02  4.75294230e-03 -8.07145025e-02\n",
      " -1.12863003e-01 -8.46867653e-02 -1.06306459e-01 -1.27754843e-01\n",
      " -2.86403250e-01 -3.05012903e-01 -6.46873780e-02 -8.15479887e-03\n",
      " -5.51633435e-02  1.63153038e-01  1.62840735e-01  2.88692298e-01\n",
      "  2.93209786e-01  1.91098640e-01  9.99813699e-02  1.01215679e-01\n",
      " -5.45778470e-02  5.02846198e-02 -3.06882576e-02  9.71282252e-03\n",
      "  4.19011317e-02  5.21583660e-02  6.68543201e-02  4.74020672e-02\n",
      "  3.00011395e-02  1.61842781e-01  1.42118397e-01  1.49040823e-01\n",
      "  2.05173190e-01  1.89021891e-01  1.70512350e-01  2.06740225e-01\n",
      "  1.62637623e-01  2.10966155e-01  2.18765679e-01  1.82210128e-01\n",
      "  2.36291393e-01  2.19408591e-01  1.22577885e-01  7.36795670e-02\n",
      "  3.52279788e-03 -8.08123466e-02 -1.98535795e-02 -5.42235133e-02\n",
      "  6.89785636e-04  4.82615061e-02  3.64626413e-02  6.73575366e-02\n",
      "  8.69152524e-02  1.04498342e-01  9.06358837e-02  7.83923936e-02\n",
      "  1.06379586e-01  4.94390415e-02  4.36573743e-02 -3.45465747e-02\n",
      " -4.62672483e-02  6.92172491e-03 -2.25027609e-02 -1.56845403e-01\n",
      " -8.34607175e-02 -1.11575720e-01 -1.80329563e-02 -5.94847015e-02\n",
      " -1.84034369e-01 -4.57988241e-01 -1.62536563e-01 -1.65796791e-01\n",
      " -1.50733488e-01 -2.36322936e-02 -5.24437129e-02  5.98061931e-03\n",
      "  1.31116705e-01 -1.87842565e-03  4.41303660e-02  2.71420969e-02\n",
      " -3.54374731e-02 -8.38054132e-03  7.01602363e-02  1.30233572e-01\n",
      "  1.27879185e-01  1.25915307e-01  1.63701763e-01  1.09597398e-01\n",
      "  9.84671614e-02  6.78353226e-03  3.55049545e-02  4.75289477e-02\n",
      "  1.99191758e-02  4.48413180e-02  7.60055490e-02  9.71373842e-02\n",
      "  1.23717492e-01  2.05345783e-01  2.26023116e-01  2.40903736e-01\n",
      "  1.88037033e-01  1.66545282e-01  1.46912591e-01  1.36693324e-01\n",
      "  4.58279904e-02  6.05910388e-02  3.31271968e-02  5.49612044e-02\n",
      "  3.45225890e-02 -1.42708771e-02  2.78413438e-02  3.32456650e-02\n",
      " -3.98813334e-02 -4.84105064e-02  2.36323602e-02  3.26578616e-02\n",
      "  1.45593965e-01  1.50208872e-01  1.65807728e-01  1.97961641e-01\n",
      "  2.20443228e-01  1.69230541e-01  1.27048340e-01  9.46170276e-02\n",
      "  9.28161894e-02  9.11027804e-03 -8.90914970e-02 -1.60807537e-01\n",
      " -2.44928211e-01 -7.06259176e-02 -7.66499642e-02 -1.97046288e-02\n",
      "  9.24972909e-02  1.41503621e-01  1.83892480e-01  2.59618316e-01\n",
      "  2.00124624e-01  1.77854511e-01  1.13649278e-01  2.07842742e-02\n",
      "  6.59947248e-02  1.13841327e-01  1.38690645e-01  1.64763937e-01\n",
      "  1.97592158e-01  1.65347809e-01  2.14472913e-01  2.47996465e-01\n",
      "  2.06208802e-01  1.75979405e-01  1.57743712e-01  1.55907256e-01\n",
      "  3.08077529e-01  4.28280921e-01  3.37598223e-01  2.70361439e-01\n",
      "  2.67362863e-01  3.11723584e-01  3.15737795e-01  1.29002572e-01\n",
      " -8.06387346e-02  6.42404938e-02  2.21526973e-01  1.92541587e-01\n",
      "  1.13915309e-01  2.03243572e-02  1.30168545e-02 -4.81620462e-02\n",
      " -2.80158765e-01 -3.26425928e-01 -3.74484769e-01 -3.62710849e-01\n",
      " -1.04156573e-01 -4.96322253e-02 -8.19087393e-02  5.62492324e-02\n",
      " -1.34880561e-01  1.62894521e-02 -2.70480040e-02 -7.46023588e-02\n",
      " -4.19380304e-02  6.36378789e-02  1.59115293e-01  2.61014435e-01\n",
      "  1.40870030e-01  9.97290723e-02 -2.64946490e-02 -1.52070686e-02\n",
      " -1.63615748e-01 -2.46691844e-01 -2.20526177e-01 -1.96456579e-01\n",
      " -9.77076369e-02 -2.70214525e-01 -3.38263947e-01 -1.68271619e-01\n",
      " -2.04455778e-01 -1.13262838e-01 -1.87231145e-01 -2.64229650e-01\n",
      " -5.72393361e-02  1.50360054e-02 -1.91395634e-02 -3.90282328e-02\n",
      " -1.27483458e-01  1.57980028e-02  3.23111788e-02 -7.54084086e-02\n",
      " -1.11367661e-01 -2.27779389e-01 -2.65056762e-01 -2.46955011e-01\n",
      " -4.23980399e-01]\n"
     ]
    }
   ],
   "source": [
    "# number of evalutaion data points\n",
    "eval_size = number_of_items - test_size\n",
    "\n",
    "# evaluation input data\n",
    "XEvaluation = []\n",
    "# evaluation output data\n",
    "YEvaluation = []\n",
    "\n",
    "# appending evaluation data to appropriate arrays\n",
    "for i in range(eval_size):\n",
    "    # finding the day we want to predict a week out\n",
    "    predict = rawdata.head(days + i + test_size + predict_delta).tail(1).as_matrix().transpose().tolist()[0][0]\n",
    "    \n",
    "    # finding the last day out of the 5 we are using as input to later calculate the percent difference \n",
    "    # of the change.  The percentage change is not the absolute value as we want to calculate negative percentages as well\n",
    "    last_day = rawdata.head(days + i + test_size).tail(1).as_matrix().transpose().tolist()[0][0]\n",
    "    \n",
    "    #result after running the AvgPrevious function on the 5 days used as input\n",
    "    res = rawdata.head(days + i + test_size).tail(days).as_matrix().transpose().tolist()[0]\n",
    "    \n",
    "    XEvaluation.append(res)\n",
    "    YEvaluation.append((predict- last_day)/predict)\n",
    "    \n",
    "XEvaluation = np.array(XEvaluation)\n",
    "YEvaluation = np.array(YEvaluation)\n",
    "\n",
    "print(XEvaluation)\n",
    "print(YEvaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1533/1533 [==============================] - 1s 621us/step - loss: 1.1033 - acc: 0.3607\n",
      "Epoch 2/10\n",
      "1533/1533 [==============================] - 0s 162us/step - loss: 1.0929 - acc: 0.3777\n",
      "Epoch 3/10\n",
      "1533/1533 [==============================] - 0s 147us/step - loss: 1.0956 - acc: 0.3705\n",
      "Epoch 4/10\n",
      "1533/1533 [==============================] - 0s 138us/step - loss: 1.0897 - acc: 0.3907\n",
      "Epoch 5/10\n",
      "1533/1533 [==============================] - 0s 164us/step - loss: 1.0908 - acc: 0.3862\n",
      "Epoch 6/10\n",
      "1533/1533 [==============================] - 0s 140us/step - loss: 1.0888 - acc: 0.3855\n",
      "Epoch 7/10\n",
      "1533/1533 [==============================] - 0s 148us/step - loss: 1.0881 - acc: 0.3894\n",
      "Epoch 8/10\n",
      "1533/1533 [==============================] - 0s 191us/step - loss: 1.0894 - acc: 0.3868\n",
      "Epoch 9/10\n",
      "1533/1533 [==============================] - 0s 149us/step - loss: 1.0844 - acc: 0.4012\n",
      "Epoch 10/10\n",
      "1533/1533 [==============================] - 0s 152us/step - loss: 1.0887 - acc: 0.3992\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(48,input_dim=5))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(1.0))\n",
    "model.add(Dense(48, activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "H = model.fit(Xtrain, Ytrain, epochs=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5155729  0.22694626 0.2574809 ]]\n"
     ]
    }
   ],
   "source": [
    "Test = [8070.8, 8891.21, 8516.24, 9477.84, 10016.49]\n",
    "XVal = np.array([Test])\n",
    "r = model.predict(XVal)\n",
    "\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dropout_19 to have shape (3,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-ecc59347b5c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXEvaluation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYEvaluation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The evaluation loss is: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mb:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[0;32m   1002\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m                                    \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                    steps=steps)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mb:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[0;32m   1766\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1769\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `_test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mb:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1480\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32mb:\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    121\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dropout_19 to have shape (3,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(XEvaluation,YEvaluation,batch_size=100, verbose=1)\n",
    "test_loss = score[0]\n",
    "print(\"The evaluation loss is: \" + str(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_84 (Dense)             (None, 48)                288       \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 24)                1176      \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 3,891\n",
      "Trainable params: 3,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5155729  0.22694626 0.2574809 ]]\n",
      "[[11490.5, 11188.6, 11474.9, 11607.4, 12899.2], -0.12748345818000645, 11440.7]\n"
     ]
    }
   ],
   "source": [
    "rawdata = pd.read_csv('data/bitcoin.csv',usecols=[3,8])\n",
    "data = rawdata.as_matrix()\n",
    "#print(np.argwhere(data=='2016-10-08')[0][0])\n",
    "\n",
    "res = getEvalData(data,'2018-01-21',predict_delta)\n",
    "\n",
    "evaluation = np.array([res[0]])\n",
    "print(model.predict(evaluation))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classifier_working.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
